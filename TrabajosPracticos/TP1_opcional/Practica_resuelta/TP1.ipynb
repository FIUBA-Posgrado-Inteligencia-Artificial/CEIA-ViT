{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordatorio: si se está trabajando en un contenedor recien instalado, va a ser necesario instalar git-hub todas las librerias requeridas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision Transformer (ViT)\n",
    "\n",
    "El **Vision Transformer (ViT)** es un modelo innovador para la clasificación de imágenes que transforma las imágenes en secuencias de parches más pequeños, comúnmente de $16 \\times 16$ píxeles, como se describe en el paper de [Alexey Dosovitskiy et al.](https://openreview.net/pdf?id=YicbFdNTTy)  \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\". Cada parche se considera como una \"palabra\" o \"token\" y se proyecta en un espacio de características. Mediante la incorporación de cifrados de posición y un token de clasificación, podemos aplicar un Transformer de manera convencional a esta secuencia, permitiendo su entrenamiento para tareas de clasificación de imágenes.\n",
    "\n",
    "## Embeddings y Cifrado de Posición\n",
    "\n",
    "### Embeddings\n",
    "En el contexto de ViTs, las imágenes son primero divididas en pequeños bloques o patches (como si se cortaran en pequeños cuadrados). Estos patches se tratan como si fueran palabras en un modelo de lenguaje, y a cada uno se le asigna un embedding. Un embedding es simplemente una representación numérica del patch en un espacio de alta dimensión que captura características relevantes del mismo.\n",
    "\n",
    "Por ejemplo, si divides una imagen de 224x224 píxeles en 16x16 bloques, tendrás 196 patches en total. Cada uno de esos patches se convierte en un vector de embedding, que luego se alimenta al modelo Transformer.\n",
    "\n",
    "### Positional Embedding (Cifrado Posicional):\n",
    "\n",
    "El **embedding posicional** agrega información sobre la ubicación de cada parche en la imagen original. En el paper de Alexey Dosovitskiy et al., se evaluaron varias formas de codificar la información espacial con embebidos posicionales. Se observó que, aunque el uso de embebidos mejora el rendimiento en comparación con no usarlos, no hay diferencias significativas entre los métodos probados. Se concluyó que, dado que el modelo opera a nivel de parches en lugar de píxeles, la forma específica de codificar la posición es menos relevante.\n",
    "## Proceso de Creación de Embeddings\n",
    "\n",
    "1. **División en Parches**: La imagen se divide en parches de $N \\times N$ píxeles.\n",
    "   \n",
    "2. **Generación de Embeddings**: Cada parche se transforma en un embedding que captura su información relevante.\n",
    "\n",
    "3. **Cifrado de Posición**: Se añade un vector de cifrado de posición a cada embedding. \n",
    "\n",
    "   Algunos ejemplos del cifrado:\n",
    "\n",
    "\n",
    "   *  **Codificación Sinusoidal:** La fórmula sinusoidal para embebidos posicionales (tradicional en Transformers-NLP) genera los valores de los embebidos en función de senos y cosenos. Estos valores dependen directamente de la posición y siguen una periodicidad específica para capturar relaciones posicionales. Es determinista y no se ajusta durante el entrenamiento, ya que los valores están calculados y fijos para cada posición.\n",
    "   Donde:\n",
    "       - $pos$ es la posición en la secuencia.\n",
    "       - $i$ es el índice de la dimensión.\n",
    "       - $d_{model}$ es la dimensión del modelo.\n",
    "\n",
    "   $$PE(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)$$\n",
    "\n",
    "   $$PE(pos, 2i + 1) = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)$$\n",
    "\n",
    "\n",
    "\n",
    "   * **Embebido Posicional Aprendido (a ser implementada por el alumno):** los embebidos posicionales se definen como un parámetro de la red neuronal (usando [nn.Parameter](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html)), lo que significa que los valores para el embebido  posicional se aprenden durante el proceso de entrenamiento. Se define embedding, matriz de tamaño (1, num_patches, embed_dim), donde num_patches es el número de parches (posiciones) en la secuencia de la imagen y  embed_dim es la dimensión de los embebidos. La inicialización torch.randn genera valores aleatorios para estos embebidos posicionales, y luego el modelo los optimiza durante el entrenamiento, ajustando los valores en función de los gradientes.\n",
    "\n",
    "4. **Combinación de Embeddings y Cifrado de Posición**: Se suma cada embedding con su correspondiente cifrado de posición, generando un vector final que contiene tanto la información del contenido del parche como su posición en la imagen.\n",
    "\n",
    "El resultado es una serie de vectores, cada uno representando un parche y su posición, que se alimentan a las capas del Transformer. Esto permite al modelo aprender no solo sobre las características individuales de cada parche, sino también sobre cómo estos se relacionan entre sí en el contexto de la imagen completa.\n",
    "\n",
    "\n",
    "![Vision Transformer](vit.gif)\n",
    "\n",
    "*Crédito: [Phil Wang](https://github.com/lucidrains/vit-pytorch/blob/main/images/vit.gif)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, trabajaremos con los **patch embeddings** y la **codificación posicional**, esenciales para que el modelo pueda interpretar las relaciones espaciales en las imágenes. \n",
    "\n",
    "## Tareas:\n",
    "\n",
    "1. **Modificar los parametros:** Cambiar el tamaño de los parches, y la cantidad de dimensiones del embedding. Investigar y describir las ventajas y desventajas de tener más o menos parches/dimensiones.\n",
    "\n",
    "2. **Implementar embedding posicional aprendido 1D:** Partiendo de ejemplo proporcionado (codificación sinusoidal en clase PositionalEncoding), implementar una clase PositionalEncodingLearned que utilice PyTorch y genere embebidos posicionales aprendidos. Graficar. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import math\n",
    "\n",
    "# La configuración, carga y preprocesamiento\n",
    "class ConfigPreprocess:\n",
    "    def __init__(self, img_path: str, img_size: int, patch_size: int):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "        print(f'Dispositivo utilizado: {self.device}')\n",
    "        \n",
    "        self.img_path = img_path\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.test_img = self.load_image()\n",
    "\n",
    "    def load_image(self):\n",
    "        return TF.to_tensor(Image.open(self.img_path).resize((self.img_size, self.img_size))).unsqueeze(0).to(self.device)\n",
    "\n",
    "    def extract_patches(self, image: Tensor) -> Tensor:\n",
    "        patches = image.unfold(1, self.patch_size, self.patch_size).unfold(2, self.patch_size, self.patch_size)\n",
    "        patches = patches.contiguous().view(image.shape[0], -1, self.patch_size, self.patch_size)\n",
    "        return patches\n",
    "    \n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size: int, patch_size: int, in_channels: int = 3, embed_dim: int = 8):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # (B, embed_dim, H/patch_size, W/patch_size)\n",
    "        x = x.flatten(2)  # (B, embed_dim, num_patches)\n",
    "        x = x.transpose(1, 2)  # (B, num_patches, embed_dim)\n",
    "        return x\n",
    "\n",
    "#class PositionalEncoding(nn.Module):\n",
    "#    def __init__(self, num_patches, embed_dim):\n",
    "#        super(PositionalEncoding, self).__init__()\n",
    "#        self.register_buffer('pos_embedding', self.create_positional_encoding(num_patches, embed_dim))\n",
    "\n",
    "#    def create_positional_encoding(self, num_patches, embed_dim):\n",
    "#        position = torch.arange(num_patches, dtype=torch.float).unsqueeze(1)\n",
    "#        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * -(math.log(10000.0) / embed_dim))\n",
    "#        pos_encoding = torch.zeros(num_patches, embed_dim)\n",
    "#        pos_encoding[:, 0::2] = torch.sin(position * div_term)  # Aplicar seno en dimensiones pares\n",
    "#        pos_encoding[:, 1::2] = torch.cos(position * div_term)  # Aplicar coseno en dimensiones impares\n",
    "#        return pos_encoding.unsqueeze(0)  # Añadir dimensión de batch\n",
    "\n",
    "#    def forward(self, x):\n",
    "#        return x + self.pos_embedding\n",
    "    \n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, num_patches, embed_dim):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, embed_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pos_embedding\n",
    "\n",
    "class Visualization:\n",
    "    @staticmethod  # No requiere self\n",
    "    def visualize_patches(patches: Tensor):\n",
    "        num_patches = patches.shape[1]\n",
    "        num_cols = int(num_patches ** 0.5)\n",
    "        num_rows = (num_patches + num_cols - 1) // num_cols\n",
    "        _, axs = plt.subplots(num_rows, num_cols, figsize=(4, 4))\n",
    "        for i in range(num_rows):\n",
    "            for j in range(num_cols):\n",
    "                idx = i * num_cols + j\n",
    "                if idx < num_patches:\n",
    "                    patch = patches[:, idx]\n",
    "                    num_channels = patch.shape[0]\n",
    "                    if num_channels == 1:\n",
    "                        axs[i, j].imshow(patch.squeeze().detach().cpu().numpy(), cmap='gray')\n",
    "                    elif num_channels == 3:\n",
    "                        axs[i, j].imshow(patch.permute(1, 2, 0).detach().cpu().numpy())\n",
    "                    axs[i, j].axis('off')\n",
    "                else:\n",
    "                    axs[i, j].axis('off')\n",
    "        plt.tight_layout(pad=0.15)\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod  # No requiere self\n",
    "    def visualize_positional_encoding(pos_embeddings: Tensor):\n",
    "        plt.figure(figsize=(14, 4))\n",
    "        plt.title('Codificaciones Posicionales para los Parches', fontsize=16, weight='bold')\n",
    "        for i in range(pos_embeddings.shape[2]):\n",
    "            plt.plot(pos_embeddings[0, :, i].detach().cpu().numpy(), label=f'Dimensión {i + 1}')\n",
    "        plt.xlabel('Índice del Parche', fontsize=14)\n",
    "        plt.ylabel('Valor de la Codificación Posicional', fontsize=14)\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def visualize_single_patch_encoding(pos_embeddings: Tensor, patch_idx: int):\n",
    "        num_patches = pos_embeddings.shape[1]\n",
    "        if patch_idx < 0 or patch_idx >= num_patches:\n",
    "            raise ValueError(f\"El índice del parche debe estar entre 0 y {num_patches - 1}, pero se recibió {patch_idx}.\")\n",
    "        patch_encoding = pos_embeddings[0, patch_idx, :].detach().cpu().numpy()\n",
    "        plt.figure(figsize=(14, 4))\n",
    "        plt.plot(patch_encoding, marker='o', label=f'Parche {patch_idx + 1}')\n",
    "        plt.title(f'Encoding Posicional para el Parche {patch_idx + 1}', fontsize=16, weight='bold')\n",
    "        plt.xlabel('Dimensión del Embedding', fontsize=14)\n",
    "        plt.ylabel('Valor del Encoding', fontsize=14)\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Parámetros\n",
    "img_path = \"raccoon.jpg\"\n",
    "img_size = 900\n",
    "patch_size = 64\n",
    "embed_dim = 8\n",
    "patch_idx = 0  # El índice del parche para el cual queres visualiizar la codificación posicional\n",
    "\n",
    "# Preprocesamiento\n",
    "config = ConfigPreprocess(img_path, img_size, patch_size)\n",
    "\n",
    "# Extracción de parches y visualización\n",
    "patches = config.extract_patches(config.test_img.squeeze(0))\n",
    "Visualization.visualize_patches(patches)\n",
    "\n",
    "# Generación de embeddings\n",
    "embedded_patches = PatchEmbedding(img_size, patch_size, 3, embed_dim).to(config.device)\n",
    "patches = embedded_patches(config.test_img)\n",
    "\n",
    "# Codificación posicional\n",
    "num_patches = (img_size // patch_size) ** 2\n",
    "positional_encoding = PositionalEncoding(num_patches, embed_dim).to(config.device)\n",
    "pos_embeddings = positional_encoding(patches)\n",
    "Visualization.visualize_positional_encoding(pos_embeddings)\n",
    "Visualization.visualize_single_patch_encoding(pos_embeddings, patch_idx)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
